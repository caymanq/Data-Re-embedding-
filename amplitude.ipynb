{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e6368c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AerEstimator (statevector).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_146855/1726290176.py:63: DeprecationWarning: The class ``qiskit.circuit.library.n_local.two_local.TwoLocal`` is deprecated as of Qiskit 2.1. It will be removed in Qiskit 3.0. Use the function qiskit.circuit.library.n_local instead.\n",
      "  return TwoLocal(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | loss=1.0921 | acc=0.420\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 189\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m Xb, Yb \u001b[38;5;129;01min\u001b[39;00m iterate_minibatches(X_train, Y_train, batch_size, seed=ep):\n\u001b[32m    188\u001b[39m     loss_fn = \u001b[38;5;28;01mlambda\u001b[39;00m th: batch_loss(estimator, Xb, Yb, th)\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     theta, lp, lm = \u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m train_probs = predict_proba(estimator, X_train, theta)\n\u001b[32m    192\u001b[39m train_pred  = np.argmax(train_probs, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 159\u001b[39m, in \u001b[36mSPSA.step\u001b[39m\u001b[34m(self, theta, loss_fn)\u001b[39m\n\u001b[32m    157\u001b[39m thetap = theta + ck * delta\n\u001b[32m    158\u001b[39m thetam = theta - ck * delta\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m lp = \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthetap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m lm = loss_fn(thetam)\n\u001b[32m    161\u001b[39m ghat = (lp - lm) / (\u001b[32m2.0\u001b[39m * ck) * delta\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 188\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(th)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m Xb, Yb \u001b[38;5;129;01min\u001b[39;00m iterate_minibatches(X_train, Y_train, batch_size, seed=ep):\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m         loss_fn = \u001b[38;5;28;01mlambda\u001b[39;00m th: \u001b[43mbatch_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m         theta, lp, lm = opt.step(theta, loss_fn)\n\u001b[32m    191\u001b[39m     train_probs = predict_proba(estimator, X_train, theta)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36mbatch_loss\u001b[39m\u001b[34m(estimator, Xb, Yb, theta)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_loss\u001b[39m(estimator, Xb, Yb, theta):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     probs = \u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_entropy(probs, Yb)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36mpredict_proba\u001b[39m\u001b[34m(estimator, Xb, theta)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_proba\u001b[39m(estimator, Xb, theta):\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m softmax(\u001b[43mlogits_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 132\u001b[39m, in \u001b[36mlogits_batch\u001b[39m\u001b[34m(estimator, Xb, theta)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m obs \u001b[38;5;129;01min\u001b[39;00m OBSERVABLES:\n\u001b[32m    131\u001b[39m     job = estimator.run(circuits=circuits, observables=[obs] * \u001b[38;5;28mlen\u001b[39m(circuits))\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     vals = np.asarray(\u001b[43mjob\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.values, dtype=\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m    133\u001b[39m     outs.append(vals)\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.stack(outs, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/cayma/OneDrive/Desktop/python/iris_env_wsl/lib/python3.12/site-packages/qiskit/primitives/primitive_job.py:65\u001b[39m, in \u001b[36mPrimitiveJob.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_submitted()\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_future\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Qiskit Iris QNN with Amplitude Embedding + Data Reuploading (3-class)\n",
    "# Uses assign_parameters with a safe fallback for older versions.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit.library import StatePreparation, TwoLocal\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "\n",
    "# Prefer AerEstimator if available (fast statevector); otherwise default Estimator\n",
    "try:\n",
    "    from qiskit_aer.primitives import Estimator as AerEstimator\n",
    "    EstimatorImpl = AerEstimator\n",
    "    print(\"Using AerEstimator (statevector).\")\n",
    "except Exception:\n",
    "    from qiskit.primitives import Estimator\n",
    "    EstimatorImpl = Estimator\n",
    "    print(\"Using default Estimator.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load & preprocess Iris data\n",
    "# -----------------------------\n",
    "X, y = load_iris(return_X_y=True)              # X: (150,4), y: {0,1,2}\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "def one_hot(y, K=3):\n",
    "    oh = np.zeros((len(y), K), dtype=float)\n",
    "    oh[np.arange(len(y)), y.astype(int)] = 1.0\n",
    "    return oh\n",
    "\n",
    "Y_train = one_hot(y_train, 3)\n",
    "Y_test  = one_hot(y_test, 3)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2) Quantum model: amplitude + reuploading\n",
    "# -----------------------------------------\n",
    "n_qubits = 2                 # 2^2 = 4 amplitudes ← 4 Iris features\n",
    "D = 2**n_qubits              # 4\n",
    "L = 5                        # reuploading depth\n",
    "\n",
    "Z0 = SparsePauliOp.from_list([(\"ZI\", 1.0)])\n",
    "Z1 = SparsePauliOp.from_list([(\"IZ\", 1.0)])\n",
    "ZZ = SparsePauliOp.from_list([(\"ZZ\", 1.0)])\n",
    "OBSERVABLES = [Z0, Z1, ZZ]   # 3 logits → 3 classes after softmax\n",
    "\n",
    "def l2_normalize_4(v):\n",
    "    v = np.asarray(v, dtype=float).reshape(-1)\n",
    "    assert v.shape[0] == 4, \"This demo expects 4 features.\"\n",
    "    n = np.linalg.norm(v)\n",
    "    return v / (n if n > 0 else 1.0)\n",
    "\n",
    "def make_layer_template():\n",
    "    # One TwoLocal block with ry/rz + CZ entanglement, reps=1\n",
    "    return TwoLocal(\n",
    "        num_qubits=n_qubits,\n",
    "        rotation_blocks=[\"ry\", \"rz\"],\n",
    "        entanglement=\"cz\",\n",
    "        reps=1,\n",
    "        parameter_prefix=\"θ\"\n",
    "    )\n",
    "\n",
    "_LAYER_TEMPLATE = make_layer_template()\n",
    "N_LAYER_PARAMS = len(_LAYER_TEMPLATE.parameters)\n",
    "\n",
    "# ---- helper: robust parameter assignment across Qiskit versions ----\n",
    "def assign_or_bind(op, mapping):\n",
    "    \"\"\"Try assign_parameters; fall back to bind_parameters if needed.\"\"\"\n",
    "    try:\n",
    "        return op.assign_parameters(mapping)\n",
    "    except AttributeError:\n",
    "        # older objects may only support bind_parameters\n",
    "        return op.bind_parameters(mapping)\n",
    "\n",
    "def make_reupload_circuit(x_norm, theta):\n",
    "    \"\"\"\n",
    "    x_norm: normalized 4-vector (||x||=1) for amplitude embedding\n",
    "    theta:  length L * N_LAYER_PARAMS (concatenated layer params)\n",
    "    \"\"\"\n",
    "    assert len(theta) == L * N_LAYER_PARAMS\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    idx = 0\n",
    "\n",
    "    for _ in range(L):\n",
    "        # Uϕ(x): amplitude embedding each reupload\n",
    "        qc.append(StatePreparation(x_norm, normalize=False), range(n_qubits))\n",
    "\n",
    "        # V(ω): one variational layer (fresh copy of the template)\n",
    "        layer = _LAYER_TEMPLATE.copy()\n",
    "\n",
    "        params_slice = list(layer.parameters)\n",
    "        bind_vals = theta[idx: idx + N_LAYER_PARAMS]\n",
    "        param_map = {p: float(v) for p, v in zip(params_slice, bind_vals)}\n",
    "\n",
    "        layer_assigned = assign_or_bind(layer, param_map)\n",
    "        qc = qc.compose(layer_assigned)\n",
    "\n",
    "        idx += N_LAYER_PARAMS\n",
    "\n",
    "    return qc\n",
    "\n",
    "# -----------------------\n",
    "# 3) Loss & train routine\n",
    "# -----------------------\n",
    "def softmax(logits):\n",
    "    z = logits - np.max(logits, axis=1, keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e / np.sum(e, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(probs, targets, eps=1e-9):\n",
    "    return -np.mean(np.sum(targets * np.log(probs + eps), axis=1))\n",
    "\n",
    "# Evaluate model logits for a batch using Estimator\n",
    "def logits_batch(estimator, Xb, theta):\n",
    "    circuits = []\n",
    "    for x in Xb:\n",
    "        x_norm = l2_normalize_4(x)\n",
    "        circuits.append(make_reupload_circuit(x_norm, theta))\n",
    "\n",
    "    # Evaluate each observable separately; stack into (B, 3)\n",
    "    outs = []\n",
    "    for obs in OBSERVABLES:\n",
    "        job = estimator.run(circuits=circuits, observables=[obs] * len(circuits))\n",
    "        vals = np.asarray(job.result().values, dtype=float)\n",
    "        outs.append(vals)\n",
    "    return np.stack(outs, axis=1)\n",
    "\n",
    "def predict_proba(estimator, Xb, theta):\n",
    "    return softmax(logits_batch(estimator, Xb, theta))\n",
    "\n",
    "def batch_loss(estimator, Xb, Yb, theta):\n",
    "    probs = predict_proba(estimator, Xb, theta)\n",
    "    return cross_entropy(probs, Yb)\n",
    "\n",
    "# -----------------------\n",
    "# 4) SPSA optimizer (simple)\n",
    "# -----------------------\n",
    "class SPSA:\n",
    "    def __init__(self, a=0.2, c=0.2, alpha=0.602, gamma=0.101, seed=1):\n",
    "        self.a0, self.c0, self.alpha, self.gamma = a, c, alpha, gamma\n",
    "        self.k = 0\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def step(self, theta, loss_fn):\n",
    "        k = self.k + 1\n",
    "        ak = self.a0 / (k ** self.alpha)\n",
    "        ck = self.c0 / (k ** self.gamma)\n",
    "        delta = self.rng.choice([-1.0, 1.0], size=theta.shape)\n",
    "        thetap = theta + ck * delta\n",
    "        thetam = theta - ck * delta\n",
    "        lp = loss_fn(thetap)\n",
    "        lm = loss_fn(thetam)\n",
    "        ghat = (lp - lm) / (2.0 * ck) * delta\n",
    "        theta_new = theta - ak * ghat\n",
    "        self.k = k\n",
    "        return theta_new, float(lp), float(lm)\n",
    "\n",
    "# -----------------------\n",
    "# 5) Train\n",
    "# -----------------------\n",
    "estimator = EstimatorImpl()\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "theta = 0.01 * rng.standard_normal(L * N_LAYER_PARAMS)\n",
    "\n",
    "opt = SPSA(a=0.2, c=0.2, alpha=0.602, gamma=0.101, seed=1)\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 20\n",
    "\n",
    "def iterate_minibatches(Xd, Yd, bs, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.permutation(len(Xd))\n",
    "    for start in range(0, len(Xd), bs):\n",
    "        j = idx[start:start+bs]\n",
    "        yield Xd[j], Yd[j]\n",
    "\n",
    "for ep in range(1, epochs + 1):\n",
    "    for Xb, Yb in iterate_minibatches(X_train, Y_train, batch_size, seed=ep):\n",
    "        loss_fn = lambda th: batch_loss(estimator, Xb, Yb, th)\n",
    "        theta, lp, lm = opt.step(theta, loss_fn)\n",
    "\n",
    "    train_probs = predict_proba(estimator, X_train, theta)\n",
    "    train_pred  = np.argmax(train_probs, axis=1)\n",
    "    train_acc   = accuracy_score(y_train, train_pred)\n",
    "    train_loss  = cross_entropy(train_probs, Y_train)\n",
    "\n",
    "    if ep == 1 or ep % 5 == 0:\n",
    "        print(f\"Epoch {ep:2d} | loss={train_loss:.4f} | acc={train_acc:.3f}\")\n",
    "\n",
    "# -----------------------\n",
    "# 6) Evaluate\n",
    "# -----------------------\n",
    "test_probs = predict_proba(estimator, X_test, theta)\n",
    "test_pred  = np.argmax(test_probs, axis=1)\n",
    "test_acc   = accuracy_score(y_test, test_pred)\n",
    "print(\"\\nTest accuracy:\", test_acc)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, test_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78972eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AerEstimator (statevector).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66488/176562901.py:76: DeprecationWarning: The class ``qiskit.circuit.library.n_local.two_local.TwoLocal`` is deprecated as of Qiskit 2.1. It will be removed in Qiskit 3.0. Use the function qiskit.circuit.library.n_local instead.\n",
      "  return TwoLocal(\n"
     ]
    }
   ],
   "source": [
    "# Qiskit Iris QNN — Amplitude Embedding + Data Reuploading\n",
    "# With stronger ansatz, wider readout, trainable classical head, SPSA training.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit.library import StatePreparation, TwoLocal\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "\n",
    "# Prefer AerEstimator if available (fast statevector); else default Estimator\n",
    "try:\n",
    "    from qiskit_aer.primitives import Estimator as AerEstimator\n",
    "    EstimatorImpl = AerEstimator\n",
    "    print(\"Using AerEstimator (statevector).\")\n",
    "except Exception:\n",
    "    from qiskit.primitives import Estimator\n",
    "    EstimatorImpl = Estimator\n",
    "    print(\"Using default Estimator.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load & preprocess Iris data\n",
    "# -----------------------------\n",
    "X, y = load_iris(return_X_y=True)              # X: (150,4), y: {0,1,2}\n",
    "\n",
    "# Standardize features (good for optimization)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "def one_hot_smooth(y, K=3, eps=0.05):\n",
    "    y = y.astype(int)\n",
    "    oh = np.full((len(y), K), eps/(K-1), dtype=float)\n",
    "    oh[np.arange(len(y)), y] = 1.0 - eps\n",
    "    return oh\n",
    "\n",
    "Y_train = one_hot_smooth(y_train, 3, eps=0.05)\n",
    "Y_test  = one_hot_smooth(y_test,  3, eps=0.05)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2) Quantum model: amplitude + reuploading\n",
    "# -----------------------------------------\n",
    "n_qubits = 2                 # 2^2 = 4 amplitudes → matches 4 Iris features\n",
    "D = 2**n_qubits              # 4\n",
    "L = 5                        # reuploading depth (↑ capacity)\n",
    "\n",
    "# Richer readout: 7 observables → feature vector (NUM_OBS = 7)\n",
    "OBSERVABLES = [\n",
    "    SparsePauliOp.from_list([(\"ZI\", 1.0)]),\n",
    "    SparsePauliOp.from_list([(\"IZ\", 1.0)]),\n",
    "    SparsePauliOp.from_list([(\"ZZ\", 1.0)]),\n",
    "    SparsePauliOp.from_list([(\"XX\", 1.0)]),\n",
    "    SparsePauliOp.from_list([(\"YY\", 1.0)]),\n",
    "    SparsePauliOp.from_list([(\"XZ\", 1.0)]),\n",
    "    SparsePauliOp.from_list([(\"ZX\", 1.0)]),\n",
    "]\n",
    "NUM_OBS = len(OBSERVABLES)\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "def l2_normalize_4(v):\n",
    "    v = np.asarray(v, dtype=float).reshape(-1)\n",
    "    assert v.shape[0] == 4, \"This demo expects 4 features.\"\n",
    "    n = np.linalg.norm(v)\n",
    "    return v / (n if n > 0 else 1.0)\n",
    "\n",
    "# TwoLocal template with stronger expressivity; robust to version quirks\n",
    "def make_layer_template():\n",
    "    try:\n",
    "        # Try a beefy template\n",
    "        return TwoLocal(\n",
    "            num_qubits=n_qubits,\n",
    "            rotation_blocks=[\"rx\", \"ry\", \"rz\"],\n",
    "            entanglement=\"full\",      # full entanglement if supported\n",
    "            reps=2,                   # deeper per layer\n",
    "            parameter_prefix=\"θ\"\n",
    "        )\n",
    "    except Exception:\n",
    "        # Fallback if \"full\" not supported in your version → use CZ ring\n",
    "        return TwoLocal(\n",
    "            num_qubits=n_qubits,\n",
    "            rotation_blocks=[\"rx\", \"ry\", \"rz\"],\n",
    "            entanglement=\"cz\",\n",
    "            reps=2,\n",
    "            parameter_prefix=\"θ\"\n",
    "        )\n",
    "\n",
    "_LAYER_TEMPLATE = make_layer_template()\n",
    "N_LAYER_PARAMS = len(_LAYER_TEMPLATE.parameters)\n",
    "\n",
    "# ---- helper: robust parameter assignment across Qiskit versions ----\n",
    "def assign_or_bind(op, mapping):\n",
    "    \"\"\"Try assign_parameters; fall back to bind_parameters if needed.\"\"\"\n",
    "    try:\n",
    "        return op.assign_parameters(mapping)\n",
    "    except AttributeError:\n",
    "        return op.bind_parameters(mapping)\n",
    "\n",
    "def make_reupload_circuit(x_norm, theta_q):\n",
    "    \"\"\"\n",
    "    x_norm: normalized 4-vector (||x||=1) for amplitude embedding\n",
    "    theta_q:  quantum parameter vector of length L * N_LAYER_PARAMS\n",
    "    \"\"\"\n",
    "    assert len(theta_q) == L * N_LAYER_PARAMS\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    idx = 0\n",
    "    for _ in range(L):\n",
    "        # Uϕ(x): amplitude re-embedding each block (data reuploading)\n",
    "        qc.append(StatePreparation(x_norm, normalize=False), range(n_qubits))\n",
    "\n",
    "        # V(ω): one variational layer (fresh template per block)\n",
    "        layer = _LAYER_TEMPLATE.copy()\n",
    "        params_slice = list(layer.parameters)\n",
    "        bind_vals = theta_q[idx: idx + N_LAYER_PARAMS]\n",
    "        param_map = {p: float(v) for p, v in zip(params_slice, bind_vals)}\n",
    "        layer_assigned = assign_or_bind(layer, param_map)\n",
    "        qc = qc.compose(layer_assigned)\n",
    "        idx += N_LAYER_PARAMS\n",
    "    return qc\n",
    "\n",
    "# -----------------------\n",
    "# 3) Classical head + utils\n",
    "# -----------------------\n",
    "TEMPERATURE = 1.5  # soften logits → smoother gradients\n",
    "\n",
    "def softmax(logits):\n",
    "    z = logits / TEMPERATURE\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e / np.sum(e, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(probs, targets, eps=1e-9):\n",
    "    return -np.mean(np.sum(targets * np.log(probs + eps), axis=1))\n",
    "\n",
    "# Parameter vector packing:\n",
    "# [ quantum θ_q | classical Wc(flat) | classical bc ]\n",
    "def split_theta(theta):\n",
    "    q_end = L * N_LAYER_PARAMS\n",
    "    c_end = q_end + NUM_CLASSES * NUM_OBS\n",
    "    theta_q = theta[:q_end]\n",
    "    Wc = theta[q_end:c_end].reshape(NUM_CLASSES, NUM_OBS)\n",
    "    bc = theta[c_end:]\n",
    "    return theta_q, Wc, bc\n",
    "\n",
    "# Evaluate logits for a batch: expectations → classical linear head\n",
    "def logits_batch(estimator, Xb, theta):\n",
    "    theta_q, Wc, bc = split_theta(theta)\n",
    "\n",
    "    circuits = []\n",
    "    for x in Xb:\n",
    "        x_norm = l2_normalize_4(x)\n",
    "        circuits.append(make_reupload_circuit(x_norm, theta_q))\n",
    "\n",
    "    # Collect expectation features (B, NUM_OBS)\n",
    "    feats = []\n",
    "    for obs in OBSERVABLES:\n",
    "        job = estimator.run(circuits=circuits, observables=[obs] * len(circuits))\n",
    "        vals = np.asarray(job.result().values, dtype=float)  # (B,)\n",
    "        feats.append(vals)\n",
    "    F = np.stack(feats, axis=1)\n",
    "\n",
    "    # Classical linear head → logits (B, NUM_CLASSES)\n",
    "    return F @ Wc.T + bc[np.newaxis, :]\n",
    "\n",
    "def predict_proba(estimator, Xb, theta):\n",
    "    return softmax(logits_batch(estimator, Xb, theta))\n",
    "\n",
    "def batch_loss(estimator, Xb, Yb, theta):\n",
    "    probs = predict_proba(estimator, Xb, theta)\n",
    "    return cross_entropy(probs, Yb)\n",
    "\n",
    "# -----------------------\n",
    "# 4) SPSA optimizer (hardware-friendly)\n",
    "# -----------------------\n",
    "class SPSA:\n",
    "    def __init__(self, a=0.15, c=0.15, alpha=0.602, gamma=0.2, seed=1):\n",
    "        self.a0, self.c0, self.alpha, self.gamma = a, c, alpha, gamma\n",
    "        self.k = 0\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def step(self, theta, loss_fn):\n",
    "        k = self.k + 1\n",
    "        ak = self.a0 / (k ** self.alpha)\n",
    "        ck = self.c0 / (k ** self.gamma)\n",
    "        delta = self.rng.choice([-1.0, 1.0], size=theta.shape)\n",
    "        thetap = theta + ck * delta\n",
    "        thetam = theta - ck * delta\n",
    "        lp = loss_fn(thetap)\n",
    "        lm = loss_fn(thetam)\n",
    "        ghat = (lp - lm) / (2.0 * ck) * delta\n",
    "        theta_new = theta - ak * ghat\n",
    "        self.k = k\n",
    "        return theta_new, float(lp), float(lm)\n",
    "\n",
    "# -----------------------\n",
    "# 5) Train\n",
    "# -----------------------\n",
    "estimator = EstimatorImpl()\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "# Quantum params\n",
    "theta_q0 = 0.01 * rng.standard_normal(L * N_LAYER_PARAMS)\n",
    "# Classical head params (Wc, bc)\n",
    "Wc0 = 0.01 * rng.standard_normal(NUM_CLASSES * NUM_OBS)\n",
    "bc0 = 0.01 * rng.standard_normal(NUM_CLASSES)\n",
    "\n",
    "theta = np.concatenate([theta_q0, Wc0, bc0])\n",
    "\n",
    "opt = SPSA(a=0.15, c=0.15, alpha=0.602, gamma=0.2, seed=1)\n",
    "\n",
    "# Longer training, smaller batches, multiple steps per batch\n",
    "epochs = 120\n",
    "batch_size = 8\n",
    "steps_per_batch = 3\n",
    "\n",
    "def iterate_minibatches(Xd, Yd, bs, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.permutation(len(Xd))\n",
    "    for start in range(0, len(Xd), bs):\n",
    "        j = idx[start:start+bs]\n",
    "        yield Xd[j], Yd[j]\n",
    "\n",
    "best_loss, patience, wait = float('inf'), 15, 0\n",
    "best_theta = theta.copy()\n",
    "\n",
    "for ep in range(1, epochs + 1):\n",
    "    for Xb, Yb in iterate_minibatches(X_train, Y_train, batch_size, seed=ep):\n",
    "        loss_fn = lambda th: batch_loss(estimator, Xb, Yb, th)\n",
    "        for _ in range(steps_per_batch):\n",
    "            theta, _, _ = opt.step(theta, loss_fn)\n",
    "\n",
    "    # Train metrics each epoch\n",
    "    train_probs = predict_proba(estimator, X_train, theta)\n",
    "    train_pred  = np.argmax(train_probs, axis=1)\n",
    "    train_acc   = accuracy_score(y_train, train_pred)\n",
    "    train_loss  = cross_entropy(train_probs, Y_train)\n",
    "\n",
    "    # Early stopping on training loss (you can split a val set if you prefer)\n",
    "    if train_loss + 1e-4 < best_loss:\n",
    "        best_loss, wait = train_loss, 0\n",
    "        best_theta = theta.copy()\n",
    "    else:\n",
    "        wait += 1\n",
    "\n",
    "    if ep == 1 or ep % 10 == 0:\n",
    "        print(f\"Epoch {ep:3d} | loss={train_loss:.4f} | acc={train_acc:.3f} | wait={wait}\")\n",
    "\n",
    "    if wait >= patience:\n",
    "        print(f\"Early stop at epoch {ep}.\")\n",
    "        theta = best_theta\n",
    "        break\n",
    "\n",
    "# -----------------------\n",
    "# 6) Evaluate\n",
    "# -----------------------\n",
    "test_probs = predict_proba(estimator, X_test, theta)\n",
    "test_pred  = np.argmax(test_probs, axis=1)\n",
    "test_acc   = accuracy_score(y_test, test_pred)\n",
    "print(\"\\nTest accuracy:\", test_acc)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, test_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f33fa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AerEstimator (statevector).\n",
      "Epoch  1 | loss=0.9195 | acc=0.732\n",
      "Epoch 10 | loss=0.6682 | acc=0.804\n",
      "Epoch 20 | loss=0.5852 | acc=0.804\n",
      "Epoch 30 | loss=0.5572 | acc=0.821\n",
      "Epoch 40 | loss=0.5307 | acc=0.812\n",
      "Epoch 50 | loss=0.5108 | acc=0.848\n",
      "Epoch 60 | loss=0.4949 | acc=0.848\n",
      "Epoch 70 | loss=0.4922 | acc=0.839\n",
      "Epoch 80 | loss=0.4770 | acc=0.839\n",
      "\n",
      "Test accuracy: 0.9210526315789473\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        13\n",
      "           1      0.917     0.846     0.880        13\n",
      "           2      0.846     0.917     0.880        12\n",
      "\n",
      "    accuracy                          0.921        38\n",
      "   macro avg      0.921     0.921     0.920        38\n",
      "weighted avg      0.923     0.921     0.921        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "\n",
    "# Prefer AerEstimator if available\n",
    "try:\n",
    "    from qiskit_aer.primitives import Estimator as AerEstimator\n",
    "    EstimatorImpl = AerEstimator\n",
    "    print(\"Using AerEstimator (statevector).\")\n",
    "except Exception:\n",
    "    from qiskit.primitives import Estimator\n",
    "    EstimatorImpl = Estimator\n",
    "    print(\"Using default Estimator.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load & preprocess Iris data\n",
    "# -----------------------------\n",
    "X, y = load_iris(return_X_y=True)              # X: (150,4), y: {0,1,2}\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "def one_hot(y, K=3):\n",
    "    oh = np.zeros((len(y), K), dtype=float)\n",
    "    oh[np.arange(len(y)), y.astype(int)] = 1.0\n",
    "    return oh\n",
    "\n",
    "Y_train = one_hot(y_train, NUM_CLASSES)\n",
    "Y_test  = one_hot(y_test,  NUM_CLASSES)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2) Model: amp embed once + angle reupload\n",
    "# -----------------------------------------\n",
    "n_qubits = 2\n",
    "L = 3  # number of data-reupload + variational blocks\n",
    "\n",
    "def l2_normalize_4(v):\n",
    "    v = np.asarray(v, dtype=float).reshape(-1)\n",
    "    assert v.shape[0] == 4, \"This demo expects 4 features.\"\n",
    "    n = np.linalg.norm(v)\n",
    "    return v / (n if n > 0 else 1.0)\n",
    "\n",
    "# Observables → 7D feature vector\n",
    "OBSERVABLES = [\n",
    "    SparsePauliOp.from_list([(\"ZI\", 1.0)]),\n",
    "    SparsePauliOp.from_list([(\"IZ\", 1.0)]),\n",
    "    SparsePauliOp.from_list([(\"ZZ\", 1.0)]),\n",
    "    SparsePauliOp.from_list([(\"XX\", 1.0)]),\n",
    "    SparsePauliOp.from_list([(\"YY\", 1.0)]),\n",
    "    SparsePauliOp.from_list([(\"XZ\", 1.0)]),\n",
    "    SparsePauliOp.from_list([(\"ZX\", 1.0)]),\n",
    "]\n",
    "NUM_OBS = len(OBSERVABLES)\n",
    "\n",
    "# Parameter layout:\n",
    "# [ theta_q (4*L) | Wr (4*4) | Wc (NUM_CLASSES * NUM_OBS) | bc (NUM_CLASSES) ]\n",
    "N_Q = 4 * L\n",
    "N_R = 4 * 4\n",
    "N_W = NUM_CLASSES * NUM_OBS\n",
    "N_B = NUM_CLASSES\n",
    "TOTAL_PARAMS = N_Q + N_R + N_W + N_B\n",
    "\n",
    "def split_theta(theta):\n",
    "    theta = np.asarray(theta, dtype=float)\n",
    "    assert theta.shape[0] == TOTAL_PARAMS\n",
    "\n",
    "    q = theta[:N_Q]\n",
    "    r = theta[N_Q:N_Q+N_R].reshape(4, 4)\n",
    "    w = theta[N_Q+N_R:N_Q+N_R+N_W].reshape(NUM_CLASSES, NUM_OBS)\n",
    "    b = theta[N_Q+N_R+N_W:]\n",
    "    return q, r, w, b\n",
    "\n",
    "def make_amp_angle_circuit(x_raw, theta_q, Wr):\n",
    "    \"\"\"\n",
    "    x_raw: (4,) standardized features (not normalized)\n",
    "    theta_q: (N_Q,) quantum variational params\n",
    "    Wr: (4,4) reupload weight matrix\n",
    "    \"\"\"\n",
    "    x_raw = np.asarray(x_raw, dtype=float).reshape(4)\n",
    "    x_norm = l2_normalize_4(x_raw)\n",
    "\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "\n",
    "    # Amplitude embedding ONCE\n",
    "    qc.prepare_state(x_norm, range(n_qubits))\n",
    "\n",
    "    # Apply L blocks of: data reupload via angles + variational block\n",
    "    idx = 0\n",
    "    for _ in range(L):\n",
    "        # ---- data reupload via learnable linear combos ----\n",
    "        # alpha = Wr @ x_raw  (shape (4,))\n",
    "        alpha = Wr @ x_raw        # 4 reupload angles\n",
    "\n",
    "        # qubit 0: RY, RZ; qubit 1: RY, RZ\n",
    "        qc.ry(float(alpha[0]), 0)\n",
    "        qc.rz(float(alpha[1]), 0)\n",
    "        qc.ry(float(alpha[2]), 1)\n",
    "        qc.rz(float(alpha[3]), 1)\n",
    "\n",
    "        # ---- variational block with 4 params ----\n",
    "        p0, p1, p2, p3 = theta_q[idx:idx+4]\n",
    "        idx += 4\n",
    "\n",
    "        qc.ry(float(p0), 0)\n",
    "        qc.rz(float(p1), 0)\n",
    "        qc.ry(float(p2), 1)\n",
    "        qc.rz(float(p3), 1)\n",
    "\n",
    "        # Entangling gate\n",
    "        qc.cx(0, 1)\n",
    "\n",
    "    return qc\n",
    "\n",
    "# -----------------------\n",
    "# 3) Loss & helpers\n",
    "# -----------------------\n",
    "def softmax(logits):\n",
    "    z = logits - np.max(logits, axis=1, keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e / np.sum(e, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(probs, targets, eps=1e-9):\n",
    "    return -np.mean(np.sum(targets * np.log(probs + eps), axis=1))\n",
    "\n",
    "def features_batch(estimator, Xb, theta):\n",
    "    theta_q, Wr, Wc, bc = split_theta(theta)\n",
    "    circuits = [make_amp_angle_circuit(x, theta_q, Wr) for x in Xb]\n",
    "\n",
    "    feats = []\n",
    "    for obs in OBSERVABLES:\n",
    "        job = estimator.run(circuits=circuits, observables=[obs] * len(circuits))\n",
    "        vals = np.asarray(job.result().values, dtype=float)  # (B,)\n",
    "        feats.append(vals)\n",
    "    F = np.stack(feats, axis=1)  # (B, NUM_OBS)\n",
    "    return F, Wc, bc\n",
    "\n",
    "def logits_batch(estimator, Xb, theta):\n",
    "    F, Wc, bc = features_batch(estimator, Xb, theta)\n",
    "    return F @ Wc.T + bc[np.newaxis, :]\n",
    "\n",
    "def predict_proba(estimator, Xb, theta):\n",
    "    return softmax(logits_batch(estimator, Xb, theta))\n",
    "\n",
    "def batch_loss(estimator, Xb, Yb, theta):\n",
    "    probs = predict_proba(estimator, Xb, theta)\n",
    "    return cross_entropy(probs, Yb)\n",
    "\n",
    "# -----------------------\n",
    "# 4) SPSA optimizer\n",
    "# -----------------------\n",
    "class SPSA:\n",
    "    def __init__(self, a=0.15, c=0.1, alpha=0.602, gamma=0.101, seed=1):\n",
    "        self.a0, self.c0, self.alpha, self.gamma = a, c, alpha, gamma\n",
    "        self.k = 0\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def step(self, theta, loss_fn):\n",
    "        k = self.k + 1\n",
    "        ak = self.a0 / (k ** self.alpha)\n",
    "        ck = self.c0 / (k ** self.gamma)\n",
    "        delta = self.rng.choice([-1.0, 1.0], size=theta.shape)\n",
    "        thetap = theta + ck * delta\n",
    "        thetam = theta - ck * delta\n",
    "        lp = loss_fn(thetap)\n",
    "        lm = loss_fn(thetam)\n",
    "        ghat = (lp - lm) / (2.0 * ck) * delta\n",
    "        theta_new = theta - ak * ghat\n",
    "        self.k = k\n",
    "        return theta_new, float(lp), float(lm)\n",
    "\n",
    "# -----------------------\n",
    "# 5) Train\n",
    "# -----------------------\n",
    "estimator = EstimatorImpl()\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "theta_init = 0.05 * rng.standard_normal(TOTAL_PARAMS)\n",
    "theta = theta_init.copy()\n",
    "\n",
    "opt = SPSA(a=0.15, c=0.1, alpha=0.602, gamma=0.101, seed=1)\n",
    "\n",
    "epochs = 80\n",
    "batch_size = 8\n",
    "steps_per_batch = 1\n",
    "\n",
    "def iterate_minibatches(Xd, Yd, bs, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.permutation(len(Xd))\n",
    "    for start in range(0, len(Xd), bs):\n",
    "        j = idx[start:start+bs]\n",
    "        yield Xd[j], Yd[j]\n",
    "\n",
    "best_loss, best_theta = float('inf'), theta.copy()\n",
    "\n",
    "for ep in range(1, epochs + 1):\n",
    "    for Xb, Yb in iterate_minibatches(X_train, Y_train, batch_size, seed=ep):\n",
    "        loss_fn = lambda th: batch_loss(estimator, Xb, Yb, th)\n",
    "        for _ in range(steps_per_batch):\n",
    "            theta, _, _ = opt.step(theta, loss_fn)\n",
    "\n",
    "    train_probs = predict_proba(estimator, X_train, theta)\n",
    "    train_pred  = np.argmax(train_probs, axis=1)\n",
    "    train_acc   = accuracy_score(y_train, train_pred)\n",
    "    train_loss  = cross_entropy(train_probs, Y_train)\n",
    "\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        best_theta = theta.copy()\n",
    "\n",
    "    if ep == 1 or ep % 10 == 0:\n",
    "        print(f\"Epoch {ep:2d} | loss={train_loss:.4f} | acc={train_acc:.3f}\")\n",
    "\n",
    "theta = best_theta  # use best params\n",
    "\n",
    "# -----------------------\n",
    "# 6) Evaluate\n",
    "# -----------------------\n",
    "test_probs = predict_proba(estimator, X_test, theta)\n",
    "test_pred  = np.argmax(test_probs, axis=1)\n",
    "test_acc   = accuracy_score(y_test, test_pred)\n",
    "print(\"\\nTest accuracy:\", test_acc)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, test_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6351848c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AerEstimator.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 193\u001b[39m\n\u001b[32m    190\u001b[39m rng = np.random.default_rng(\u001b[32m0\u001b[39m)\n\u001b[32m    191\u001b[39m theta0 = \u001b[32m0.05\u001b[39m * rng.standard_normal(TOTAL)\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m theta_opt = \u001b[43madam_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# 6) Evaluate\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    199\u001b[39m test_probs = predict_prob(X_test, theta_opt)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 168\u001b[39m, in \u001b[36madam_train\u001b[39m\u001b[34m(theta, X, Y, epochs, batch_size, lr)\u001b[39m\n\u001b[32m    165\u001b[39m j = idx[k:k+batch_size]\n\u001b[32m    166\u001b[39m Xb, Yb = X[j], Y[j]\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m g = \u001b[43mgrad_est\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m m = b1*m + (\u001b[32m1\u001b[39m-b1)*g\n\u001b[32m    171\u001b[39m v = b2*v + (\u001b[32m1\u001b[39m-b2)*(g*g)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 159\u001b[39m, in \u001b[36madam_train.<locals>.grad_est\u001b[39m\u001b[34m(theta, Xb, Yb, h)\u001b[39m\n\u001b[32m    157\u001b[39m     tp = theta.copy(); tp[i]+=h\n\u001b[32m    158\u001b[39m     tm = theta.copy(); tm[i]-=h\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     g[i] = (loss_fn(Xb, Yb, tp) - \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtm\u001b[49m\u001b[43m)\u001b[49m)/(\u001b[32m2\u001b[39m*h)\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m g\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mloss_fn\u001b[39m\u001b[34m(Xb, Yb, theta)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss_fn\u001b[39m(Xb, Yb, theta):\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     p = \u001b[43mpredict_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -np.mean(np.sum(Yb*np.log(p+\u001b[32m1e-9\u001b[39m),axis=\u001b[32m1\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 136\u001b[39m, in \u001b[36mpredict_prob\u001b[39m\u001b[34m(Xb, theta)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_prob\u001b[39m(Xb, theta):\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     F, Wc, bc = \u001b[43mfeatures_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     logits = F @ Wc.T + bc\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m softmax(logits)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 124\u001b[39m, in \u001b[36mfeatures_batch\u001b[39m\u001b[34m(Xb, theta)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m obs \u001b[38;5;129;01min\u001b[39;00m OBS:\n\u001b[32m    123\u001b[39m     job = estimator.run(circuits=circuits, observables=[obs]*\u001b[38;5;28mlen\u001b[39m(circuits))\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     vals = np.asarray(\u001b[43mjob\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.values)\n\u001b[32m    125\u001b[39m     feats.append(vals)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.stack(feats, axis=\u001b[32m1\u001b[39m), Wc, bc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/cayma/OneDrive/Desktop/python/iris_env_wsl/lib/python3.12/site-packages/qiskit/primitives/primitive_job.py:65\u001b[39m, in \u001b[36mPrimitiveJob.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_submitted()\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_future\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "\n",
    "# Prefer statevector simulator\n",
    "try:\n",
    "    from qiskit_aer.primitives import Estimator as AerEstimator\n",
    "    EstimatorImpl = AerEstimator\n",
    "    print(\"Using AerEstimator.\")\n",
    "except:\n",
    "    from qiskit.primitives import Estimator\n",
    "    EstimatorImpl = Estimator\n",
    "    print(\"Using default Estimator.\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load & preprocess Iris data\n",
    "# -----------------------------\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "def one_hot(y, K=3):\n",
    "    oh = np.zeros((len(y), K))\n",
    "    oh[np.arange(len(y)), y] = 1\n",
    "    return oh\n",
    "\n",
    "Y_train = one_hot(y_train, 3)\n",
    "Y_test  = one_hot(y_test,  3)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Model config\n",
    "# -----------------------------\n",
    "n_qubits = 2\n",
    "L = 3                                   # reupload depth\n",
    "OBS = [\n",
    "    SparsePauliOp.from_list([(\"ZI\",1)]),\n",
    "    SparsePauliOp.from_list([(\"IZ\",1)]),\n",
    "    SparsePauliOp.from_list([(\"ZZ\",1)]),\n",
    "    SparsePauliOp.from_list([(\"XX\",1)]),\n",
    "    SparsePauliOp.from_list([(\"YY\",1)]),\n",
    "    SparsePauliOp.from_list([(\"XZ\",1)]),\n",
    "    SparsePauliOp.from_list([(\"ZX\",1)]),\n",
    "]\n",
    "NUM_OBS = len(OBS)\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# total parameters:\n",
    "# Q = 4*L variational\n",
    "# R = 4x4 reupload matrix\n",
    "# Wc, bc = classical head\n",
    "N_Q = 4*L\n",
    "N_R = 16\n",
    "N_W = NUM_CLASSES*NUM_OBS\n",
    "N_B = NUM_CLASSES\n",
    "TOTAL = N_Q + N_R + N_W + N_B\n",
    "\n",
    "\n",
    "def split_params(theta):\n",
    "    q = theta[:N_Q]\n",
    "    r = theta[N_Q:N_Q+N_R].reshape(4,4)\n",
    "    w = theta[N_Q+N_R:N_Q+N_R+N_W].reshape(NUM_CLASSES, NUM_OBS)\n",
    "    b = theta[N_Q+N_R+N_W:]\n",
    "    return q, r, w, b\n",
    "\n",
    "\n",
    "def l2norm(x):\n",
    "    n = np.linalg.norm(x)\n",
    "    return x/n if n>0 else x\n",
    "\n",
    "\n",
    "def make_circuit(x_raw, qparams, Wr):\n",
    "    x_raw = np.asarray(x_raw)\n",
    "    x_norm = l2norm(x_raw)\n",
    "\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "\n",
    "    # amplitude embed ONCE\n",
    "    qc.prepare_state(x_norm, range(n_qubits))\n",
    "\n",
    "    idx = 0\n",
    "    for _ in range(L):\n",
    "        # --- reupload ---\n",
    "        alpha = Wr @ x_raw\n",
    "        qc.ry(alpha[0], 0)\n",
    "        qc.rz(alpha[1], 0)\n",
    "        qc.ry(alpha[2], 1)\n",
    "        qc.rz(alpha[3], 1)\n",
    "\n",
    "        # --- variational ---\n",
    "        p0, p1, p2, p3 = qparams[idx:idx+4]\n",
    "        idx += 4\n",
    "\n",
    "        qc.ry(p0, 0)\n",
    "        qc.rz(p1, 0)\n",
    "        qc.ry(p2, 1)\n",
    "        qc.rz(p3, 1)\n",
    "        qc.cx(0,1)\n",
    "\n",
    "    return qc\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Forward pass\n",
    "# -----------------------------\n",
    "estimator = EstimatorImpl()\n",
    "\n",
    "def features_batch(Xb, theta):\n",
    "    qparams, Wr, Wc, bc = split_params(theta)\n",
    "    circuits = [make_circuit(x, qparams, Wr) for x in Xb]\n",
    "\n",
    "    feats = []\n",
    "    for obs in OBS:\n",
    "        job = estimator.run(circuits=circuits, observables=[obs]*len(circuits))\n",
    "        vals = np.asarray(job.result().values)\n",
    "        feats.append(vals)\n",
    "    return np.stack(feats, axis=1), Wc, bc\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z,axis=1,keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e/np.sum(e,axis=1,keepdims=True)\n",
    "\n",
    "\n",
    "def predict_prob(Xb, theta):\n",
    "    F, Wc, bc = features_batch(Xb, theta)\n",
    "    logits = F @ Wc.T + bc\n",
    "    return softmax(logits)\n",
    "\n",
    "\n",
    "def loss_fn(Xb, Yb, theta):\n",
    "    p = predict_prob(Xb, theta)\n",
    "    return -np.mean(np.sum(Yb*np.log(p+1e-9),axis=1))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) ADAM optimizer!!\n",
    "# -----------------------------\n",
    "def adam_train(theta, X, Y, epochs=60, batch_size=12, lr=0.015):\n",
    "    m = np.zeros_like(theta)\n",
    "    v = np.zeros_like(theta)\n",
    "    b1, b2, eps = 0.9, 0.999, 1e-8\n",
    "\n",
    "    def grad_est(theta, Xb, Yb, h=1e-3):\n",
    "        g = np.zeros_like(theta)\n",
    "        for i in range(len(theta)):\n",
    "            tp = theta.copy(); tp[i]+=h\n",
    "            tm = theta.copy(); tm[i]-=h\n",
    "            g[i] = (loss_fn(Xb, Yb, tp) - loss_fn(Xb, Yb, tm))/(2*h)\n",
    "        return g\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        idx = np.random.permutation(len(X))\n",
    "        for k in range(0, len(X), batch_size):\n",
    "            j = idx[k:k+batch_size]\n",
    "            Xb, Yb = X[j], Y[j]\n",
    "\n",
    "            g = grad_est(theta, Xb, Yb)\n",
    "\n",
    "            m = b1*m + (1-b1)*g\n",
    "            v = b2*v + (1-b2)*(g*g)\n",
    "\n",
    "            mhat = m/(1-b1**ep)\n",
    "            vhat = v/(1-b2**ep)\n",
    "\n",
    "            theta -= lr*mhat/(np.sqrt(vhat)+eps)\n",
    "\n",
    "        if ep % 10 == 0:\n",
    "            train_p = predict_prob(X, theta)\n",
    "            acc = accuracy_score(y_train, np.argmax(train_p,axis=1))\n",
    "            L = loss_fn(X, Y, theta)\n",
    "            print(f\"Epoch {ep:2d} | Loss: {L:.4f} | Acc: {acc:.3f}\")\n",
    "\n",
    "    return theta\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Train\n",
    "# -----------------------------\n",
    "rng = np.random.default_rng(0)\n",
    "theta0 = 0.05 * rng.standard_normal(TOTAL)\n",
    "\n",
    "theta_opt = adam_train(theta0, X_train, Y_train, epochs=60, batch_size=12)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Evaluate\n",
    "# -----------------------------\n",
    "test_probs = predict_prob(X_test, theta_opt)\n",
    "test_pred  = np.argmax(test_probs, axis=1)\n",
    "\n",
    "print(\"\\nTest accuracy:\", accuracy_score(y_test, test_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, test_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb9ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "\n",
    "try:\n",
    "    from qiskit_aer.primitives import Estimator as AerEstimator\n",
    "    EstimatorImpl = AerEstimator\n",
    "except:\n",
    "    from qiskit.primitives import Estimator\n",
    "    EstimatorImpl = Estimator\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Data\n",
    "# -----------------------------\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "def one_hot(y):\n",
    "    oh = np.zeros((len(y),NUM_CLASSES))\n",
    "    oh[np.arange(len(y)),y] = 1\n",
    "    return oh\n",
    "\n",
    "Y_train = one_hot(y_train)\n",
    "Y_test  = one_hot(y_test)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Model config\n",
    "# -----------------------------\n",
    "n_qubits = 2\n",
    "L = 2                                    # amplitude reupload depth\n",
    "OBS = [\n",
    "    SparsePauliOp.from_list([(\"ZI\",1)]),\n",
    "    SparsePauliOp.from_list([(\"IZ\",1)]),\n",
    "    SparsePauliOp.from_list([(\"ZZ\",1)]),\n",
    "    SparsePauliOp.from_list([(\"XX\",1)]),\n",
    "    SparsePauliOp.from_list([(\"YY\",1)]),\n",
    "    SparsePauliOp.from_list([(\"XZ\",1)]),\n",
    "    SparsePauliOp.from_list([(\"ZX\",1)])\n",
    "]\n",
    "NUM_OBS = len(OBS)\n",
    "\n",
    "\n",
    "N_Q = 4*L\n",
    "N_W = NUM_CLASSES*NUM_OBS\n",
    "N_B = NUM_CLASSES\n",
    "TOTAL = N_Q + N_W + N_B\n",
    "\n",
    "\n",
    "def split(theta):\n",
    "    q = theta[:N_Q]\n",
    "    w = theta[N_Q:N_Q+N_W].reshape(NUM_CLASSES, NUM_OBS)\n",
    "    b = theta[N_Q+N_W:]\n",
    "    return q, w, b\n",
    "\n",
    "\n",
    "def l2norm(x):\n",
    "    n = np.linalg.norm(x)\n",
    "    return x/n if n>0 else x\n",
    "\n",
    "\n",
    "def make_amp_reupload(xvec, qparams):\n",
    "    xnorm = l2norm(xvec)\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    idx = 0\n",
    "\n",
    "    for _ in range(L):\n",
    "        qc.prepare_state(xnorm, range(n_qubits))\n",
    "\n",
    "        p0,p1,p2,p3 = qparams[idx:idx+4]\n",
    "        idx += 4\n",
    "\n",
    "        qc.ry(p0,0); qc.rz(p1,0)\n",
    "        qc.ry(p2,1); qc.rz(p3,1)\n",
    "        qc.cx(0,1)\n",
    "\n",
    "    return qc\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Forward pass\n",
    "# -----------------------------\n",
    "estimator = EstimatorImpl()\n",
    "\n",
    "def features_batch(Xb, theta):\n",
    "    qparams, Wc, bc = split(theta)\n",
    "    circuits = [make_amp_reupload(x, qparams) for x in Xb]\n",
    "\n",
    "    feats = []\n",
    "    for obs in OBS:\n",
    "        job = estimator.run(circuits=circuits, observables=[obs]*len(circuits))\n",
    "        vals = np.asarray(job.result().values)\n",
    "        feats.append(vals)\n",
    "    return np.stack(feats,axis=1), Wc, bc\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z,axis=1,keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e/np.sum(e,axis=1,keepdims=True)\n",
    "\n",
    "\n",
    "def predict_prob(Xb, theta):\n",
    "    F, Wc, bc = features_batch(Xb, theta)\n",
    "    logits = F @ Wc.T + bc\n",
    "    return softmax(logits)\n",
    "\n",
    "\n",
    "def loss_fn(Xb, Yb, theta):\n",
    "    p = predict_prob(Xb, theta)\n",
    "    return -np.mean(np.sum(Yb*np.log(p+1e-9),axis=1))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Adam optimizer\n",
    "# -----------------------------\n",
    "def adam(theta, X, Y, epochs=60, batch=12, lr=0.02):\n",
    "    m = np.zeros_like(theta)\n",
    "    v = np.zeros_like(theta)\n",
    "    b1, b2, eps = 0.9, 0.999, 1e-8\n",
    "\n",
    "    def grad(theta, Xb, Yb, h=1e-3):\n",
    "        g = np.zeros_like(theta)\n",
    "        for i in range(len(theta)):\n",
    "            tp = theta.copy(); tp[i]+=h\n",
    "            tm = theta.copy(); tm[i]-=h\n",
    "            g[i] = (loss_fn(Xb,Yb,tp) - loss_fn(Xb,Yb,tm))/(2*h)\n",
    "        return g\n",
    "\n",
    "    for ep in range(1,epochs+1):\n",
    "        idx = np.random.permutation(len(X))\n",
    "        for k in range(0,len(X),batch):\n",
    "            j = idx[k:k+batch]\n",
    "            Xb,Yb = X[j],Y[j]\n",
    "\n",
    "            g = grad(theta, Xb, Yb)\n",
    "\n",
    "            m = b1*m + (1-b1)*g\n",
    "            v = b2*v + (1-b2)*(g*g)\n",
    "\n",
    "            mhat = m/(1-b1**ep)\n",
    "            vhat = v/(1-b2**ep)\n",
    "\n",
    "            theta -= lr*mhat/(np.sqrt(vhat)+eps)\n",
    "\n",
    "        if ep%10==0:\n",
    "            L = loss_fn(X,Y,theta)\n",
    "            acc = accuracy_score(y_train, np.argmax(predict_prob(X,theta),axis=1))\n",
    "            print(f\"[B2] Epoch {ep} | Loss={L:.4f} | Acc={acc:.3f}\")\n",
    "\n",
    "    return theta\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Train\n",
    "# -----------------------------\n",
    "rng = np.random.default_rng(0)\n",
    "theta0 = 0.05*rng.standard_normal(TOTAL)\n",
    "theta_opt = adam(theta0, X_train, Y_train, epochs=60, batch=12)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate\n",
    "# -----------------------------\n",
    "test_probs = predict_prob(X_test, theta_opt)\n",
    "test_pred  = np.argmax(test_probs,axis=1)\n",
    "\n",
    "print(\"\\n[B2] Test accuracy:\", accuracy_score(y_test,test_pred))\n",
    "print(\"\\n[B2] Classification report:\\n\", classification_report(y_test,test_pred,digits=3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iris_env_wsl (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
